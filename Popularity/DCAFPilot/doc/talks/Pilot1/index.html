<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>reveal.js - The HTML Presentation Framework</title>

    <meta name="description" content="DCAFPilot for CMS">
    <meta name="author" content="Valentin Kuznetsov">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.min.css">
    <!-- beige.css,moon.css,serif.css,sky.css,default.css,night.css,simple.css,solarized.css -->
    <link rel="stylesheet" href="css/theme/default.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- local styles-->
    <style type="text/css" media="all">
    .highlight {color:#FF8000; padding: 30px;}
    </style>

    <!-- If the query includes 'print-pdf', use the PDF print sheet -->
    <script>
        document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">

<!-- Any section element inside of this container is displayed as a slide -->
<div class="slides">

<section>
    <h2>DCAFPilot for CMS</h2>
    <h3>pilot project for CMS computing data-mining</h3>
    <p>
    <small>by
        <a href="http://github.com/vkuznet">Valentin Kuznetsov</a> /
        <a href="http://www.cornell.edu">Cornell University</a>
    </small>
    </p>
</section>

<section>
    <h2>Talk outlines</h2>
    <ul>
        <li>Project outlines</li>
        <li>From data to prediction</li>
        <ul>
            <li>gather information from data-services</li>
            <li>prepare information suitable for data analysis</li>
            <li>train learner algorithm</li>
            <li>make prediction</li>
        </ul>
        <li>Future direction</li>
    </ul>
</section>

<section>
<div align="left">
<h2>There are too many numbers ...</h2>
<ul>
<li>
DMWM analytics group would like to improve our understanding of CMS computing data
</li>
<li>
<a href="https://github.com/dmwm/DMWMAnalytics/tree/master/Popularity/DCAFPilot">DCAFPilot</a>
(Data and Computing Analysis Framework) is a pilot project to get started.
</li>

<li>
Ultimately we'd like to learn from CMS data and make prediction to improve our resource utilization.
</li>

<li>
Initial goal was to predict popularity of datasets on weekly basis.
</li>

<li>
Start with understanding metrics, analysis workflow, tools, etc.
</li>

</ul>

</div>
</section>

<section>
<h2>Project outlines</h2>
<div align="left">
    <p>
The DCAFPilot consists of several components:
<small>
    <ul>
    <li>Dataframe generator toolkit: collect/transform data from
    CMS data-services (DBS/Phedex/SiteDB/PopularityDB/Dashboard)
    and extract necessary bits for datasets in questions
    </li>
    <li>Machine Learning (ML) algorithms (python/R code) for data analysis</li>
    <li>Scripts for data manipulation</li>
    </ul>
</small>
</p>

<p>
Setup and dependencies:
<small>
    <ul>
        <li><a href="https://github.com/dmwm/DMWMAnalytics/tree/master/Popularity/DCAFPilot">DCAFPilot</a> project, available at github</li>
        <li><a href="http://www.mongodb.org">MongoDB</a> for internal cache</li>
        <li>Python tools: <a href="http://pandas.pydata.org/">pandas</a>,
            <a href="http://www.numpy.org/">NumPy</a>, <a href="http://www.scipy.org/">SciPy</a>,
            <a href="http://scikit-learn.org/stable">sklearn</a> ML toolkit</li>
        <li><a href="http://www.r-project.org">R language</a> (for data exploration, statistical computing and ML algorithms)</li>
        <li>Optional: <a href="https://github.com/EducationalTestingService/skll">SKLL</a> toolkit to run/experiment with various ML algorithms, <a href="https://github.com/JohnLangford/vowpal_wabbit">Vowpal Wabbit</a> online-learning algorithm</li>
    </ul>
</small>
Get the code:
<pre style="width:1000px">
git clone git@github.com:dmwm/DMWMAnalytics.git
</pre>
</p>

</div>
</section>

<section>
<h2>Data collection flow</h2>
<div align="left">
We collect data via the following set of rules
<small>
    <ul>
    <li>Divide workflow on weekly basis</li>
    <li>Collect all datasets (4 DBS instances) into internal cache</li>
    <li>Collection popular datasets from PopularityDB</li>
    <li>Get summary for datasets from DBS/Phedex/SiteDB/Dashboard data-services</li>
    <li>Complement dataframe with random set of DBS datasets which were not
    visible in popularity for given time internval</li>
    </ul>
</small>
</br>

List of used CMS data-service APIs used by DCAFPilot package
</br>
<small>
    <ul>
        <li><a href="https://cmsweb.cern.ch/dbs/">DBS:</a> datasets, releases, filesummaries, releaseversions, datasetparents APIs</li>
        <li><a href="https://cmsweb.cern.ch/phedex">Phedex:</a> blockReplicas API</li>
        <li><a href="https://cmsweb.cern.ch/sitedb">SiteDB:</a> site-names, people APIs</li>
        <li><a href="https://cms-popularity.cern.ch/popdb/popularity/apidoc">PopularityDB:</a> DSStatInTimeWindow API</li>
        <li><a href="https://twiki.cern.ch/twiki/bin/view/ITAnalyticsWorkingGroup/ExperimentDashboard">Dashboard:</a> jobefficiencyapi API</li>
    </ul>
</small>
</div>
</section>

<section>
<h2>Dataframe preparation</h2>
<div align="left">
<small>
    <span class="highlight">DCAFPilot tools:</span>
    <code>generate_scripts.sh, dataframe</code>
    <pre style="width:1000px">
    <code>
# generate run-time scripts starting from given date
prompt$ bin/generate_scripts.sh 20140101 > run.sh

# content of run.sh
prompt$ head run.sh
#!/bin/bash
dataframe --seed-cache
nohup time dataframe --start=20140101 --stop=20140108 --dbs-extra=1000 --verbose=1 --fout=dataframe-20140101-20140108.csv  2>&1 1>& dataframe-20140101-20140108.log < /dev/null &
...

# run dataframe generator
prompt$ nohup time dataframe --start=20141126 --stop=20141203 --dbs-extra=1000 --verbose=1 --fout=dataframe-20141126-20141203.csv 2>&1 1>& dataframe-20141126-20141203.log < /dev/null &

# check log content
prompt$ head dataframe-20141126-20141203.log
dbs::fetch url=https://cmsweb.cern.ch/dbs/prod/global/DBSReader//releaseversions, params={'dataset': '/ZMM/Summer11-DESIGN42_V11_428_SLHC1-v1/GEN-SIM'}, docid=0f6c2bcc3970ab7812cfa17f1b92c930
phedex::fetch url=https://cmsweb.cern.ch/phedex/datasvc/json/prod/blockReplicas, params={'dataset': '/ZMM/Summer11-DESIGN42_V11_428_SLHC1-v1/GEN-SIM'}, docid=7c718caf86d31108c57fcf0cad47a722
dbs::fetch url=https://cmsweb.cern.ch/dbs/prod/global/DBSReader//datasetparents, params={'dataset': '/ZMM/Summer11-DESIGN42_V11_428_SLHC1-v1/GEN-SIM'}, docid=d7c76de195d92e4b885376a25db28964
dbs::fetch url=https://cmsweb.cern.ch/dbs/prod/global/DBSReader//datasetparents, params={'dataset': '/ZMM/Summer11-DESIGN42_V11_428_SLHC1-v1/GEN-SIM'}, docid=d7c76de195d92e4b885376a25db28964
dbs::fetch url=https://cmsweb.cern.ch/dbs/prod/global/DBSReader//filesummaries, params={'dataset': '/ZMM/Summer11-DESIGN42_V11_428_SLHC1-v1/GEN-SIM'}, docid=5f41d7d0855411af390e1d2c641c9aa0
dashboard::fetch url=http://dashb-cms-job.cern.ch/dashboard/request.py/jobefficiencyapi, params={'start': '2014-11-26', 'type': 'analysis', 'end': '2014-12-3', 'site': 'all', 'dataset': '/ZMM/Summer11-DESIGN42_V11_428_SLHC1-v1/GEN-SIM'}
popdb::fetch url=https://cms-popularity.cern.ch/popdb/popularity//DSStatInTimeWindow?tstop=2014-12-3&tstart=2014-11-26, params={'tstop': '2014-12-3', 'tstart': '2014-11-26'}

    </code>
    </pre>
</small>
</div>
</section>

<section>
<h2>Dataframe preparation, cont'd</h2>
<div align="left">
    <pre style="width:1000px">
Queried 5 CMS data-services: DBS, Phedex, SiteDB, PopularityDB, Dashboard
    - used 10 APIs to get data content
    - feed internal cache with ~217K datasets from 4 DBS istances,
      ~900 release names, 500+ site names, ~5k people DNs.

Placed ~610K queries, ~345K went to data-services and the rest used internal cache

The final dataframe is constructed out of 71 variables and has 46 files and ~114K rows
   - each file is worth of 1 week of CMS data

The average size of the object is 1KB, ~0.6GB of CMS data

Elapsed time: ~2h to 25m per job, times fade out due to cache usage
    </pre>
<small>
We anonymised all data and performed factorization via internal cache
    <pre style="width:1000px">
        <code>
id,cpu,creator,dataset,dbs,dtype,era,nblk,nevt,nfiles,nlumis,nrel,nsites,parent,primds,proc_evts,procds,rel1_0,rel1_1,rel1_2,rel1_3,rel1_4,rel1_5,rel1_6,rel1_7,rel2_0,rel2_1,rel2_10,rel2_11,rel2_2,rel2_3,rel2_4,rel2_5,rel2_6,rel2_7,rel2_8,rel2_9,rel3_0,rel3_1,rel3_10,rel3_11,rel3_12,rel3_13,rel3_14,rel3_15,rel3_16,rel3_17,rel3_18,rel3_19,rel3_2,rel3_20,rel3_21,rel3_22,rel3_23,rel3_3,rel3_4,rel3_5,rel3_6,rel3_7,rel3_8,rel3_9,relt_0,relt_1,relt_2,s_0,s_1,s_2,s_3,s_4,size,tier,wct,target
999669242,207737071,2179,20186,3,0,759090,6,21675970,2158,72274,1,10,5862538,335429,30667701,373256,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,3,8,0,0,8002,5,216946588,0.006
332990665,114683734,2179,176521,3,1,759090,88,334493030,32621,86197,1,4,6086362,968016,123342232,1037052,0,0,0,0,1,2,0,0,0,0,0,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,1,0,2,0,6,9,0,0,96689,3,276683510,0.008
....
        </code>
    </pre>
</small>

</section>

<section>
<h2>Dataframe description</h2>
<small>
<pre style="width:1000px">
<span class="highlight">id:</span> unique id constructed as long('%s%s%s'%(tstamp,dbsinst,dataset_id)) % 2**30
<span class="highlight">cpu:</span> CPU time reported by Dashboard data-service for given dataset
<span class="highlight">creator:</span> anonymized DN of the user who created given dataset, reported by DBS
<span class="highlight">dataset:</span> DBS dataset id (comes from DBS APIs/database backend)
<span class="highlight">dbs:</span> DBS instance id
<span class="highlight">dtype:</span> anonymized DBS data type (e.g. data, mc)
<span class="highlight">era:</span> anonymized DBS acquisition era name associated with given dataset
<span class="highlight">nblk:</span> number of blocks in given dataset, reported by DBS
<span class="highlight">nevt:</span> number of events in given dataset, reported by DBS
<span class="highlight">nfiles:</span> number of files in given dataset, reported by DBS
<span class="highlight">nlumis:</span> number of lumi sections in given dataset, reported by DBS
<span class="highlight">nrel:</span> number of releases associated with given dataset, reported by DBS
<span class="highlight">nsites:</span> number of sites associated with given dataset, reported by Phedex
<span class="highlight">parent:</span> parent id of given dataset, reported by DBS
<span class="highlight">primds:</span> anonymized primary dataset name, reported by DBS
<span class="highlight">proc_evts:</span> number of processed events, reported by Dashboard
<span class="highlight">procds:</span> anonymized processed dataset name, reported by DBS
<span class="highlight">rel1_N:</span> DBS release counter defined as N-number of series releases associated with given dataset
<span class="highlight">rel2_N:</span> DBS release counter defined as N-number of major releases associated with given dataset
<span class="highlight">rel3_N:</span> DBS release counter defined as N-number of minor releases associated with given dataset
<span class="highlight">s_X:</span> Phedex site counter, i.e. number of Tier sites holding this dataset replica
<span class="highlight">size:</span> size of the dataset, reported by DBS and normalized to GB metric
<span class="highlight">tier:</span> anonymized tier name, reported by DBS
<span class="highlight">wct:</span> wall clock counter for given dataset, reported by Dashboard
<span class="highlight">target:</span> target metric reported by PopularityDB, current default is normalized number of access to given dataset (other metrics can be normalized CPU and number of users associated with given dataset).
</pre>
<div align="left">
Please check README.md for each dataset release, e.g. 0.0.2 release used specific metric, while
0.0.3 release used un-normalized data and various metrics.
</div>
</small>
</section>

<section>
<h2>Live Data plots</h2>
<p><img src="images/dbs1.gif" alt="DBS plots" /></p>
Data transition through 2014 on weekly basis
</section>

<section>
<h2>Live Data plots, cont'd</h2>
<p><img src="images/dbs2.gif" alt="DBS plots" /></p>
Data transition through 2014 on weekly basis
</section>

<section>
<h2>Live Data plots, cont'd</h2>
<p><img src="images/dbs3.gif" alt="DBS plots" /></p>
Data transition through 2014 on weekly basis
</section>

<section>
<h2>Correlations</h2>
<p><img src="images/corr.gif" alt="Correlations" /></p>
Data transition through 2014 on weekly basis
</section>

<section>
<h2>What we're looking for</h2>
<p><img src="images/popularity.jpg" alt="Popularity" /></p>
<small>
Datasets behavior through 2014 (plot shows few random datasets)
</small>
</section>

<section>
<h2>How to train your model</h2>
<div align="left">
<small>
    <span class="highlight">DCAFPilot tools:</span>
    <code>model</code>
    <pre style="width:1000px"><code>
# use model script to train your model
prompt$ model --help

Usage: model.py [options]

Options:
  -h, --help            show this help message and exit
  --scaler=SCALER       model scalers: ['StandardScaler', 'MinMaxScaler'], default None
  --scorer=SCORER       model scorers: ['accuracy', 'adjusted_rand_score', 'average_precision',
                        'f1', 'log_loss', 'mean_absolute_error', 'mean_squared_error',
                        'precision', 'r2', 'recall', 'roc_auc'], default None
  --learner=LEARNER     model learners: ['AdaBoostClassifier', 'AdaBoostRegressor', 'BaggingClassifier',
                        'BaggingRegressor', 'BernoulliNB', 'DecisionTreeClassifier', 'DecisionTreeRegressor',
                        'ExtraTreesClassifier', 'ExtraTreesRegressor', 'GaussianNB', 'GradientBoostingClassifier',
                        'GradientBoostingRegressor', 'KNeighborsClassifier', 'LinearSVC', 'PCA',
                        'RandomForestClassifier', 'RandomForestRegressor', 'RidgeClassifier',
                        'SGDClassifier', 'SGDRegressor', 'SVC', 'SVR',
                        'lda_rfc', 'pca_knc', 'pca_rfc', 'pca_svc'], default RandomForestClassifier
  --learner-params=LPARAMS
                        model classifier parameters, supply via JSON
  --learner-help=LEARNER_HELP
                        Print learner description, default None
  --split=SPLIT         split level for train/validation, default 0.33
  --train-file=TRAIN    train file, default train.csv
  --newdata=NEWDATA     new data file, default None
  --idx=IDX             initial index counter, default 0
  --limit=LIMIT         number of rows to process, default -1 (everything)
  --verbose=VERBOSE     verbose output, default=0
  --crossval            Perform cross-validation for given model and quit
  --gsearch=GSEARCH     perform grid search, gsearch=<parameters>
  --predict=PREDICT     Prediction file name, default None
    </code></pre>
<span class="highlight">Note:</span>
model script offers 8 regressors and 18 classifiers from
<a href="http://scikit-learn.org/stable/">sklearn</a> ML library
</small>
</div>
</section>

<section>
<h2>Learn your learner</h2>
<div align="left">
<small>
    <pre style="width:1000px"><code>
prompt$ model --learner-help=RandomForestRegressor

RandomForestRegressor(bootstrap=True, compute_importances=None,
           criterion='mse', max_depth=None, max_features='auto',
           max_leaf_nodes=None, min_density=None, min_samples_leaf=1,
           min_samples_split=2, n_estimators=10, n_jobs=1, oob_score=False,
           random_state=None, verbose=0)
A random forest regressor.

    A random forest is a meta estimator that fits a number of classifying
    decision trees on various sub-samples of the dataset and use averaging
    to improve the predictive accuracy and control over-fitting.

    Parameters
    ----------
    n_estimators : integer, optional (default=10)
        The number of trees in the forest.

    criterion : string, optional (default="mse")
        The "function" to measure the quality of a split. The only supported
        criterion is "mse" for the mean squared error.
        Note: this parameter is tree-specific.

    max_features : int, float, string or None, optional (default="auto")
        The number of features to consider when looking for the best split:

        - If int, then consider `max_features` features at each split.
        - If float, then `max_features` is a percentage and
          `int(max_features * n_features)` features are considered at each
          split.
        - If "auto", then `max_features=n_features`.
        - If "sqrt", then `max_features=sqrt(n_features)`.
        - If "log2", then `max_features=log2(n_features)`.
        - If None, then `max_features=n_features`.
...
    </code></pre>
</small>
</div>
</section>

<section>
<h2>Analyze and play with your model</h2>
<div align="left">
    <pre style="width:1000px">
        <code>
# we can pass additional parameters to the learner, e.g. find out rank of features
prompt$ model --learner=RandomForestRegressor --train-file=2014.csv.gz --predict=pred.txt --scaler=StandardScaler --learner-params='{"random_state":123, "compute_importances":true}'
RandomForestRegressor(bootstrap=True, compute_importances=True,
           criterion='mse', max_depth=None, max_features='auto',
           max_leaf_nodes=None, min_density=None, min_samples_leaf=1,
           min_samples_split=2, n_estimators=10, n_jobs=1, oob_score=False,
           random_state=123, verbose=0)
Feature ranking:
1. importance 0.558986, feature nsites
2. importance 0.105061, feature s_2
3. importance 0.074761, feature rel3_0
4. importance 0.049544, feature proc_evts
5. importance 0.043663, feature s_1
6. importance 0.035363, feature rel3_2
7. importance 0.034260, feature s_3
8. importance 0.029350, feature rel3_4
9. importance 0.005171, feature relt_2
        </code>
Do you think site variables are good predictors?
    </pre>
</div>
</section>
<section>
<h2>First results</h2>
<div align="left">
<small>
    <span class="highlight">DCAFPilot tools:</span>
    <code>merge_csv, model</code>
</small>
    <pre style="width:1000px"><code>
# get the data, we keep it sequre in separate CERN based repository
prompt$ git clone https://:@git.cern.ch/kerberos/CMS-DMWM-Analytics-data

# merge all 2014 files into single file
prompt$ merge_csv --fin=CMS-DMWM-Analytics-data/Popularity/DCAFPilot/data/0.0.2 \
          --fout=2014.csv.gz --verbose

# Train the model
prompt$ model --learner=RandomForestRegressor --train-file=2014.csv.gz --scorer=r2

RandomForestRegressor(bootstrap=True, compute_importances=None,
           criterion='mse', max_depth=None, max_features='auto',
           max_leaf_nodes=None, min_density=None, min_samples_leaf=1,
           min_samples_split=2, n_estimators=10, n_jobs=1, oob_score=False,
           random_state=None, verbose=0)
Score metric (r2): 0.897606379597
    </code></pre>
</div>
<small>
We should hold our breath with this though, too many things to test first ...
</small>
</section>

<section>
<h2>First results, prediction</h2>
<div align="left">
<small>
    <pre style="width:1000px">
        <code>
prompt$ model --learner=RandomForestRegressor --train-file=2014.csv.gz --scorer=r2 --newdata=dataframe-20141119-20141126.csv.gz --predict=pred.txt
RandomForestRegressor(bootstrap=True, compute_importances=None,
           criterion='mse', max_depth=None, max_features='auto',
           max_leaf_nodes=None, min_density=None, min_samples_leaf=1,
           min_samples_split=2, n_estimators=10, n_jobs=1, oob_score=False,
           random_state=None, verbose=0)
Score metric (r2): 0.93623237727
Final Logloss 0.000381044756446
Traceback (most recent call last):
...
  File "/Users/vk/CMS/DMWM/GIT/DMWMAnalytics/Popularity/DCAFPilot/src/python/DCAF/ml/model.py", line 165, in model
    predictions = fit.predict(tdf)
  File "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/ensemble/forest.py", line 564, in predict
    if getattr(X, "dtype", None) != DTYPE or X.ndim != 2:
  File "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/ops.py", line 600, in wrapper
    res = na_op(values, other)
  File "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/ops.py", line 564, in na_op
    raise TypeError("invalid type comparison")
TypeError: invalid type comparison
        </code>
    </pre>
<span class="highlight">Hint:</span>
Some data requires normalization/scaling, e.g. cpu/wct were generated as int's (to save output file space), while they should be floats (fixed in 0.0.3 release).
</small>
</div>
</section>

<section>
<h2>First results, prediction w/ scaling</h2>
<div align="left">
<small>
    <span class="highlight">DCAFPilot tools:</span>
    <code>model, check_prediction</code>
    <pre style="width:1000px">
        <code>
# run model with scaler
prompt$ model --learner=RandomForestRegressor --train-file=2014.csv.gz --scorer=r2 --newdata=dataframe-20141119-20141126.csv.gz --predict=pred.txt --scaler=StandardScaler
RandomForestRegressor(bootstrap=True, compute_importances=None,
           criterion='mse', max_depth=None, max_features='auto',
           max_leaf_nodes=None, min_density=None, min_samples_leaf=1,
           min_samples_split=2, n_estimators=10, n_jobs=1, oob_score=False,
           random_state=None, verbose=0)
Score metric (r2): -4.22807249175

# check our prediction
prompt$ check_prediction --fin=dataframe-20141119-20141126.csv.gz --fpred=pred.txt

Explaied variance score: 0.898592908177
Mean absolute error    : 0.000386050647644
Mean squared error     : 2.42792029612e-06
R2 score               : 0.896133210959

Definition: R2=1-Sres/Stot, Stot=SUM_i(Y_i-Y_hat)^2, Sres=SUM_i(F_i-Y_hat)^2
        </code>
    </pre>
<span class="highlight">Observation:</span>
You may see different results if re-run your model
(random splitting for train/validation sets).
<br />
<span class="highlight">Hint:</span>
Use random seed and no splitting to have consistent results
</small>
</div>
</section>

<section>
<h2>Other learners</h2>
<div align="left">
<small>
    <pre style="width:1000px">
        <code>
# run AdaBoost regressor model
prompt$ model --learner=AdaBoostRegressor --train-file=2014.csv.gz --scorer=r2 --scaler=StandardScaler
AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',
         n_estimators=50, random_state=None)
Score metric (r2): -0.943959694892

# run Bagging regressor model
prompt$ model --learner=BaggingRegressor --train-file=2014.csv.gz --scorer=r2 --scaler=StandardScaler
BaggingRegressor(base_estimator=None, bootstrap=True,
         bootstrap_features=False, max_features=1.0, max_samples=1.0,
         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,
         verbose=0)
Score metric (r2): -2.11591876017

# run SGD regressor model
prompt$ model --learner=SGDRegressor --train-file=2014.csv.gz --scorer=r2 --scaler=StandardScaler
SGDRegressor(alpha=0.0001, epsilon=0.1, eta0=0.01, fit_intercept=True,
       l1_ratio=0.15, learning_rate='invscaling', loss='squared_loss',
       n_iter=5, penalty='l2', power_t=0.25, random_state=None,
       shuffle=False, verbose=0, warm_start=False)
Score metric (r2): -3.12784471837e+24
        </code>
    </pre>
<span class="highlight">Hint:</span>
DCAFPilot supports five regressors, and 15+ classifiers which may be
used if we'll use different metrics.
</small>
</div>
</section>

<section>
<h2>Online learning algorithm</h2>
<div align="left">
<small>
The <a href="http://hunch.net/~vw/">Vowpal wabbit</a> is online learning
algorithm started at Yahoo! Research and continuing at Microsoft Research to
design a fast, scalable, useful learning algorithm. VW is the essence of speed
in machine learning, able to learn from <u>terafeature</u> datasets with ease. Via
parallel learning, it can exceed the throughput of any single machine network
interface when doing linear learning, a first amongst learning algorithms.
<br />
<br />
<span class="highlight">DCAFPilot tools:</span>
<code>csv2vw</code>
    <pre style="width:1000px">
prompt$ csv2vw --csv=2014.csv.gz --vw=2014.vw
prompt$ head -2 2014.vw

0.006 '999669242 |f size:8002 nevt:21675970 creator:2179 wct:216946588 dtype:0 dataset:20186 procds:373256 nfiles:2158 dbs:3 rel3_8:1 rel3_22:0 rel3_20:0 rel3_21:0 rel3_7:0 rel3_6:0 rel3_5:0 rel3_4:0 rel3_3:0 rel3_2:1 rel3_1:0 rel3_0:0 nsites:10 rel3_9:0 rel3_23:0 rel2_8:0 rel2_9:0 rel2_6:0 rel2_7:0 rel2_4:0 rel2_5:0 rel2_2:1 rel2_3:1 rel2_0:0 rel2_1:0 parent:5862538 rel1_1:0 rel1_0:0 rel1_3:0 rel1_2:0 rel1_5:1 rel1_4:1 rel1_7:0 nrel:1 rel3_17:0 rel3_16:0 rel3_15:0 nblk:6 rel3_13:0 rel3_12:0 rel3_11:0 rel3_10:0 tier:5 primds:335429 rel1_6:0 rel2_10:0 rel2_11:0 rel3_19:0 rel3_18:0 s_1:3 s_0:0 s_3:0 s_2:8 s_4:0 relt_0:1 relt_1:0 relt_2:1 rel3_14:0 era:759090 nlumis:72274 proc_evts:30667701 cpu:207737071

0.008 '332990665 |f size:96689 nevt:334493030 creator:2179 wct:276683510 dtype:1 dataset:176521 procds:1037052 nfiles:32621 dbs:3 rel3_8:1 rel3_22:0 rel3_20:0 rel3_21:0 rel3_7:1 rel3_6:0 rel3_5:0 rel3_4:0 rel3_3:0 rel3_2:1 rel3_1:0 rel3_0:0 nsites:4 rel3_9:0 rel3_23:0 rel2_8:0 rel2_9:0 rel2_6:0 rel2_7:0 rel2_4:0 rel2_5:0 rel2_2:1 rel2_3:2 rel2_0:0 rel2_1:0 parent:6086362 rel1_1:0 rel1_0:0 rel1_3:0 rel1_2:0 rel1_5:2 rel1_4:1 rel1_7:0 nrel:1 rel3_17:0 rel3_16:0 rel3_15:0 nblk:88 rel3_13:0 rel3_12:0 rel3_11:0 rel3_10:0 tier:3 primds:968016 rel1_6:0 rel2_10:0 rel2_11:0 rel3_19:0 rel3_18:0 s_1:6 s_0:0 s_3:0 s_2:9 s_4:0 relt_0:1 relt_1:0 relt_2:2 rel3_14:0 era:759090 nlumis:86197 proc_evts:123342232 cpu:114683734
    </pre>
    <span class="highlight">Please note:</span>
    VW should be installed externally, here we show its data format
and how to use it. The <b>csv2vw</b> script is provided by DCAFPilot package.
</small>
</div>
</section>

<section>
<h2>Online learning algorithm, cont'd</h2>
<div align="left">
<small>
    <span class="highlight">External tools:</span>
    <code>vw</code>
    <pre style="width:1000px">
        <code>
# train our model
prompt$ vw -d 2014.vw -c -k -f model.vw
Num weight bits = 18, learning rate = 0.5, initial_t = 0, power_t = 0.5
average    since         example     example  current  current  current
loss       last          counter      weight    label  predict features
0.000036   0.000036            1         1.0   0.0060   0.0000       29
...
0.000014   0.000010        65536     65536.0   0.0010   0.0000       46

finished run, number of examples per pass = 113908, passes used = 1
average loss = 1.29019e-05
best constants loss = 0.000366653
total feature number = 6071657

# make prediction
prompt$ vw -d new.vw -t -i model.vw -p preds.txt

# convert VW prediction into CSV format
prompt$ vw_pred2csv --fin=preds.txt --fout=preds.csv

# check VW prediction against data sample
prompt$ check_prediction --fin=dataframe-20141119-20141126.csv.gz --fpred=preds.csv
        </code>
                        RandomForestRegressor   VW regression
Explaied variance score: 0.898592908177         0.341559976762
Mean absolute error    : 0.000386050647644      0.000349310257493
Mean squared error     : 2.42792029612e-06      1.54817975922e-05
R2 score               : 0.896133210959         0.337686411265
    </pre>
</small>
</div>
</section>

<section>
<h2>Regression or classification</h2>
<div align="left">
<small>
<span class="highlight">Expernal tools:</span>
<code>R-language, e1071 library (Support Vector Machine classification)</code>
    <pre style="width:1000px">
        <code>
library(e1071)

csv_files <- list.files(path="./", full.names=TRUE, pattern="dataframe-.*.csv.gz")
df <- read.csv(file=csv_files[1])
for ( i in 2:length(csv_files) ) {
  df <- rbind(df,read.csv(file=csv_files[1]))
}

# remove variables which will not be useful for predicting _new_ data popularity
df <- df[,cbind("cpu","creator","dtype","era","nblk","nevt","nfiles","nlumis",
                "parent","primds","proc_evts","procds","size","tier","wct","target")]

# define classifcation
df$target <- df$target > 0.001
dLen <- nrow(df)
nTrain <- floor(dLen * 0.8)
dTrain <- df[1:nTrain,]
dTest  <- df[nTrain+1:(dLen-nTrain-1),]

# The default cost=1 gives only 90% accuracty, 2611 support vectors.
# cost=10 gives 97% accuracy with 1012 support vectors.
# cost=100 gives 100% accuracy with only 431 support vectors.
# putting the cost to infinity (10^6) gives 285 support vectors.
s <- svm(target ~ ., data=dTrain, type="C-classification", cost=50)
print(s) # 542 support  for cost=50

yTest <- predict(s,dTest)
print("Confusion matrix for test sample")
print(table(yTest,dTest$target))
        </code>
    </pre>
</small>
</div>

</section>

<section>
<h2>From data to prediction</h2>
<div align="left">
<small>
<span class="highlight">DCAFPilot tools:</span>
<code>merge_csv, model, check_prediction, pred2dataset</code>
    <pre style="width:1000px">
        <code>
# merge dataframes
prompt$ merge_csv --fin=CMS-DMWM-Analytics-data/Popularity/DCAFPilot/data/0.0.2 \
          --fout=2014.csv.gz --verbose

# train the model
prompt$ model --learner=RandomForestRegressor --train-file=2014.csv.gz --scorer=r2 --newdata=dataframe-20141119-20141126.csv.gz --predict=pred.txt --scaler=StandardScaler --learner-params='{"random_state":123}'
RandomForestRegressor(bootstrap=True, compute_importances=None,
           criterion='mse', max_depth=None, max_features='auto',
           max_leaf_nodes=None, min_density=None, min_samples_leaf=1,
           min_samples_split=2, n_estimators=10, n_jobs=1, oob_score=False,
           random_state=123, verbose=0)
Score metric (r2): -12.6244503822

# check prediction
prompt$ check_prediction --fin=dataframe-20141119-20141126.csv.gz --fpred=pred.txt
Explaied variance score: 0.82718229836
Mean absolute error    : 0.000560324232394
Mean squared error     : 4.2115601847e-06
R2 score               : 0.819828832958

# convert prediction into human/CMS data format
prompt$ pred2dataset --fin=pred.txt --fout=pred.txt.out

# inspect prediction
prompt$ head -3 pred.txt.out
0.239,/GenericTTbar/HC-CMSSW_7_0_4_START70_V7-v1/GEN-SIM-RECO
0.007,/QCD_Pt-15to3000_Tune4C_14TeV_pythia8/SHCAL2023Upg14DR-PU140bx25_PH2_1K_FB_V4-v1/GEN-SIM-RECO
0.025,/SingleMu/Run2012D-22Jan2013-v1/AOD
        </code>
    </pre>
</small>
</div>

</section>

<section>
<h2>New data and features</h2>
<div align="left">
<small>
<ul>
    <li>We generated two datasets, tagged as 0.0.2 and 0.0.3</li>
    <li>The later contains un-normalized variables and all popularity DB metrics</li>
    <li>We added transform script for data manipulation and convertion; feature
        ranking, data splitting, etc.</li>
</ul>
    <pre style="width:1000px">
        <code>
# merge data
merge_csv --fin=/Users/vk/CMS/DMWM/GIT/CMS-DMWM-Analytics-data/Popularity/DCAFPilot/data/0.0.3/ --fout=2014.csv.gz

# transform data into classification problem
transform_csv --fin=2014.csv.gz --fout=2014_clf.csv.gz --target=naccess --target-thr=100 \
              --drops="nusers,totcpu,rnaccess,rnusers,rtotcpu,nsites,s_0,s_1,s_2,s_3,s_4,wct"

# train classification model
model --learner=RandomForestClassifier --idcol=id --target=target --train-file=2014_clf.csv.gz --scorer=precision,accuracy,f1
RandomForestClassifier(bootstrap=True, compute_importances=None,
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,
            min_samples_split=2, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0)
Split level: train 67.0%, validation 33.0%
Feature ranking:
1. importance 0.188098, feature nlumis
2. importance 0.120278, feature size
3. importance 0.100305, feature nfiles
4. importance 0.090498, feature rel3_8
5. importance 0.062367, feature nevt
6. importance 0.044238, feature rel3_18
7. importance 0.042559, feature proc_evts
8. importance 0.039234, feature rel1_1
9. importance 0.021832, feature rel2_3
Score metric (precision_score): 0.949993305663
Score metric (accuracy_score): 0.957849506369
Score metric (f1_score): 0.945121545122
        </code>
    </pre>
</small>
</div>
</section>

<section>
<h2>Regression versus classification</h2>
<div align="left">
<small>
<ul>
    <li>Samples: 118K (1K extra) vs 573K (10K extra) DBS datasets</li>
</ul>
    <pre style="width:1000px">
        <code>
                        Classification problem (call RandomForestClassifiers)
2014 small sample                                   2014 large sample
1. importance 0.188098, feature nlumis              1. importance 0.104199, feature nevt
2. importance 0.120278, feature size                2. importance 0.098317, feature nlumis
3. importance 0.100305, feature nfiles              3. importance 0.076636, feature nfiles
4. importance 0.090498, feature rel3_8              4. importance 0.063266, feature size
5. importance 0.062367, feature nevt                5. importance 0.055817, feature rel3_8
6. importance 0.044238, feature rel3_18             6. importance 0.055472, feature rel2_3
7. importance 0.042559, feature proc_evts           7. importance 0.051285, feature relt_1
8. importance 0.039234, feature rel1_1              8. importance 0.049851, feature rel2_9
9. importance 0.021832, feature rel2_3              9. importance 0.049331, feature rel2_11
Score metric (precision_score): 0.949993305663      Score metric (precision_score): 0.953751635474
Score metric (accuracy_score): 0.957849506369       Score metric (accuracy_score): 0.987040888141
Score metric (f1_score): 0.945121545122             Score metric (f1_score): 0.931936299714
Precision=TP/(TP+FP), Accuracy=(TP+TN)/(TP+TN+FP+FN), Recall=TP/(TP+FN), F1=2*Precision*Recall/(Precision+Recall)

                        Regression problem (call RandomForestRegressors)
1. importance 0.969912, feature procds              1. importance 0.688633, feature relt_1
2. importance 0.016207, feature relt_1              2. importance 0.146298, feature procds
3. importance 0.003690, feature relt_2              3. importance 0.130334, feature relt_2
4. importance 0.003170, feature rel2_9              4. importance 0.016545, feature rel3_18
5. importance 0.001091, feature rel3_14             5. importance 0.002197, feature rel3_15
6. importance 0.000901, feature rel3_12             6. importance 0.001408, feature rel3_17
7. importance 0.000631, feature rel3_17             7. importance 0.001202, feature rel3_0
8. importance 0.000564, feature rel3_19             8. importance 0.000984, feature rel3_13
9. importance 0.000312, feature rel3_7              9. importance 0.000973, feature rel3_14
Score metric (mean_squared_error): 24.0979075909    Score metric (mean_squared_error): 9.36329295342
Score metric (mean_absolute_error): 0.400884685387  Score metric (mean_absolute_error): 0.731153281794
Score metric (r2_score): 0.999975155713             Score metric (r2_score): 0.99999728645
Stot=SUM_i(Y_i-Y_hat)^2, Sres=SUM_i(F_i-Y_hat)^2, R2=1-Sres/Stot
        </code>
    </pre>
</small>
</div>
</section>

<section>
<h2>Conclusions & Future directions</h2>
<div align="left">
<small>
<ul>
    <li>We show the proof of concept how to predict dataset popularity based on existing CMS tools</li>
    <ul>
        <li>DCAFPilot package has main components to do the work</li>
        <li>Initial dataframe shows some potential but should be throughfully studied to avoid
        main ML obstucles, such as as data memorization, overfitting, etc.</li>
        <li>More data in terms of volume and attributes may be required for
        further analysis, e.g. find physicists clustering on certain topics</li>
        <li>Even though all work was done on a single node with existing APIs we may need
        to pursue other approaches, e.g. ORACLE-Hadoop mapping, etc.</li>
    </ul>
    <li>Explore various ML algorithms both within python sklearn toolkit and R</li>
    <li>Try out different popularity metrics, e.g. naccess, ncpu, nusers or any
    combination of them</li>
    <li>Test predictions with real data, i.e. acquire new datasets and make prediction for them,
    then wait for data from popularity DB and compare prediction with actual data</li>
    <li>Try out online learners, they
        may work better with time-dependent features or larger datasts</li>
    <li>Automate tools, e.g. weekly crontabs, generate model updates, verify model predictions</li>
</ul>
</small>
</div>
</section>

<section>
<h1>Happy Data Mining</h1>
</section>

</div> <!-- end of reveal -->

</div> <!-- end of slides -->

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.min.js"></script>

<script>

// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: true,

    theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Optional libraries used to extend on reveal.js
    dependencies: [
        { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
        { src: 'plugin/markdown/showdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
        { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
        // { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
        // { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
    ]
});

</script>

</body>
</html>

