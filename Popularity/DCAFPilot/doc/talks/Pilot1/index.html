<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>DCAFPilot - pilot project for CMS computing data-mining</title>

    <meta name="description" content="DCAFPilot for CMS">
    <meta name="author" content="Valentin Kuznetsov">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<!--
    <link rel="stylesheet" href="css/reveal.min.css">
-->
    <link rel="stylesheet" href="css/reveal.css">
    <!-- beige.css,moon.css,serif.css,sky.css,default.css,night.css,simple.css,solarized.css -->
    <link rel="stylesheet" href="css/theme/default.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- local styles-->
    <style type="text/css" media="all">
    .highlight {color:#FF8000; padding: 30px;}
    </style>

    <!-- If the query includes 'print-pdf', use the PDF print sheet -->
    <script>
        document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">

<!-- Any section element inside of this container is displayed as a slide -->
<div class="slides">

<section>
    <h2>DCAFPilot for CMS</h2>
    <h3>pilot project for CMS computing data-mining</h3>
    <p>
    <small>by
        <a href="http://github.com/vkuznet">Valentin Kuznetsov</a> /
        <a href="http://www.cornell.edu">Cornell University</a>
    </small>
    </p>
</section>

<section>
    <h2>Talk outlines</h2>
    <ul>
        <li>Project outlines</li>
        <li>From data to prediction
            <ul>
                <li>gather information from data-services</li>
                <li>prepare information suitable for data analysis</li>
                <li>train learner algorithm</li>
                <li>make prediction</li>
            </ul>
        </li>
        <li>Future direction</li>
    </ul>
</section>

<section>
<div align="left">
<h2>There are too many numbers ...</h2>
<ul>
<li>
DMWM-Analytics
(<a href="mailto:cms-dmwm-analytics@cern.ch">cms-dmwm-analytics@cern.ch</a>)
group would like to improve our understanding of CMS computing data,
full list of projects:
<small>
<a href="https://twiki.cern.ch/twiki/bin/viewauth/CMS/CMSComputingAnalytics">
https://twiki.cern.ch/twiki/bin/viewauth/CMS/CMSComputingAnalytics
</a>
</small>
</li>

<li>
Ultimately we'd like to learn from CMS data and make prediction to improve our resource utilization.
<br />
<br />
</li>

<li>
Initial goal is to predict popularity of new datasets.
<br />
<br />
</li>

<li>
Start with understanding metrics, analysis workflow, tools:
<ul>
    <li>
    <a href="https://github.com/dmwm/DMWMAnalytics/tree/master/Popularity/DCAFPilot">DCAFPilot</a>
    (Data and Computing Analysis Framework) is a pilot project to understand
    machinery involved with this problem.
    </li>
</ul>
</li>

</ul>

</div>
</section>

<section>
<h2>Project outlines</h2>
<div align="left">
    <p>
The DCAFPilot consists of several components:
<small>
    <ul>
    <li>Dataframe generator toolkit: collect/transform data from
    CMS data-services (DBS/PhEDEx/SiteDB/PopularityDB/Dashboard)
    and extract necessary bits for datasets in questions
    <br />
    <br />
    </li>
    <li>Machine Learning (ML) algorithms (python/R code) for data analysis
    <br />
    <br />
    </li>
    <li>Data manipulation scripts: merge, transform, check predictions, etc.</li>
    </ul>
</small>
</p>

<p>
Get the code:
<pre>
git clone git@github.com:dmwm/DMWMAnalytics.git
</pre>
</p>

<p>
Dependencies:
<small>
    <ul>
        <li><a href="https://github.com/dmwm/DMWMAnalytics/tree/master/Popularity/DCAFPilot">DCAFPilot</a> project, available at github</li>
        <li><a href="http://www.mongodb.org">MongoDB</a> for internal cache</li>
        <li>Python tools: <a href="http://pandas.pydata.org/">pandas</a>,
            <a href="http://www.numpy.org/">NumPy</a>, <a href="http://www.scipy.org/">SciPy</a>,
            <a href="http://scikit-learn.org/stable">sklearn</a> ML toolkit</li>
        <li>
        <span class="highlight">Optional:</span>
        <a href="http://www.r-project.org">R language</a> for data exploration and ML algorithms,
        <a href="https://github.com/EducationalTestingService/skll">SKLL</a>
        toolkit to run/experiment with various ML algorithms, any other external
        ML toolkits, e.g.
        <a href="https://github.com/JohnLangford/vowpal_wabbit">Vowpal Wabbit</a> online-learning algorithm</li>
    </ul>
</small>
</p>

</div>
</section>

<section>
<h2>Data collection flow</h2>
<div align="left">
<ul>
<li>
Collect data via the following set of rules
<small>
    <ul>
    <li>Collect all datasets (4 DBS instances) into internal cache</li>
    <li>Collection popular datasets from PopularityDB on weekly basis</li>
    <li>Get summary for datasets from DBS/PhEDEx/SiteDB/Dashboard data-services</li>
    <li>Complement dataframe with random set of DBS datasets which were not
    visible in popularity for given time interval</li>
    </ul>
</small>
<br />
<br />
</li>

<li>
CMS data-service APIs used by DCAFPilot package
<br />
<small>
    <ul>
        <li><a href="https://cmsweb.cern.ch/dbs/">DBS:</a> datasets, releases, filesummaries, releaseversions, datasetparents APIs</li>
        <li><a href="https://cmsweb.cern.ch/phedex">PhEDEx:</a> blockReplicas API</li>
        <li><a href="https://cmsweb.cern.ch/sitedb">SiteDB:</a> site-names, people APIs</li>
        <li><a href="https://cms-popularity.cern.ch/popdb/popularity/apidoc">PopularityDB:</a> DSStatInTimeWindow API</li>
        <li><a href="https://twiki.cern.ch/twiki/bin/view/ITAnalyticsWorkingGroup/ExperimentDashboard">Dashboard:</a> jobefficiencyapi API</li>
    </ul>
</small>

</li>
</ul>

</div>
</section>

<section>
<h2>Data collection flow diagram</h2>
<p><img src="images/DCAFPilot_flow.png" alt="Data Collection Flow" width="70%" style="background-color:#E6E6E6" /></p>
</section>

<section>
<h2>Dataframe preparation</h2>
<div align="left">
<small>
    <span class="highlight">DCAFPilot tools:</span>
    <code>generate_scripts.sh, dataframe</code>
    <pre>
    <code class="bash">
# generate run-time scripts starting from given date
prompt$ bin/generate_scripts.sh 20140101 > run.sh

# content of run.sh
prompt$ head run.sh
#!/bin/bash
dataframe --seed-cache
dataframe --start=20140101 --stop=20140108 --dbs-extra=1000 --verbose=1 --fout=dataframe-20140101-20140108.csv
...

# run dataframe generator (you may need to run it in background)
prompt$ dataframe --start=20141126 --stop=20141203 --dbs-extra=1000 --verbose=1 --fout=dataframe-20141126-20141203.csv

# check log content
prompt$ head dataframe-20141126-20141203.log
dbs::fetch url=https://cmsweb.cern.ch/dbs/prod/global/DBSReader//releaseversions, params={'dataset': '/ZMM/Summer11-DESIGN42_V11_428_SLHC1-v1/GEN-SIM'}, docid=0f6c2bcc3970ab7812cfa17f1b92c930
phedex::fetch url=https://cmsweb.cern.ch/phedex/datasvc/json/prod/blockReplicas, params={'dataset': '/ZMM/Summer11-DESIGN42_V11_428_SLHC1-v1/GEN-SIM'}, docid=7c718caf86d31108c57fcf0cad47a722
dbs::fetch url=https://cmsweb.cern.ch/dbs/prod/global/DBSReader//datasetparents, params={'dataset': '/ZMM/Summer11-DESIGN42_V11_428_SLHC1-v1/GEN-SIM'}, docid=d7c76de195d92e4b885376a25db28964
dbs::fetch url=https://cmsweb.cern.ch/dbs/prod/global/DBSReader//datasetparents, params={'dataset': '/ZMM/Summer11-DESIGN42_V11_428_SLHC1-v1/GEN-SIM'}, docid=d7c76de195d92e4b885376a25db28964
dbs::fetch url=https://cmsweb.cern.ch/dbs/prod/global/DBSReader//filesummaries, params={'dataset': '/ZMM/Summer11-DESIGN42_V11_428_SLHC1-v1/GEN-SIM'}, docid=5f41d7d0855411af390e1d2c641c9aa0
dashboard::fetch url=http://dashb-cms-job.cern.ch/dashboard/request.py/jobefficiencyapi, params={'start': '2014-11-26', 'type': 'analysis', 'end': '2014-12-3', 'site': 'all', 'dataset': '/ZMM/Summer11-DESIGN42_V11_428_SLHC1-v1/GEN-SIM'}
popdb::fetch url=https://cms-popularity.cern.ch/popdb/popularity//DSStatInTimeWindow?tstop=2014-12-3&tstart=2014-11-26, params={'tstop': '2014-12-3', 'tstart': '2014-11-26'}

    </code>
    </pre>
</small>
</div>
</section>

<section>
<h2>Dataframe preparation, cont'd</h2>
<div align="left">
    <pre>
Queried 5 CMS data-services: DBS, PhEDEx, SiteDB, PopularityDB, Dashboard
   - used 10 APIs to get data content
   - feed internal cache with ~220K datasets from 4 DBS instances,
     ~900 release names, 500+ site names, ~5k people DNs.
   - placed ~800K queries

The final dataframe is constructed out of 78 variables and has 52 files and ~600K rows
   - each file is worth of 1 week of CMS data, ~600KB zipped/file
   - each file has about ~1K of popular datasets plus 10K random "un-popular" datasets

Elapsed time: ~4h to 1h per job, times fade out due to cache usage (MongoDB)
All jobs run on two CERN VM w/ N jobs/core splitting
    </pre>
<small>
We anonymized all data and performed factorization via internal cache
    <pre>
        <code>
id,cpu,creator,dataset,dbs,dtype,era,naccess,nblk,nevt,nfiles,nlumis,nrel,nsites,nusers,parent,primds,proc_evts,procds,rel1_0,rel1_1,rel1_2,rel1_3,rel1_4,rel1_5,rel1_6,rel1_7,rel2_0,rel2_1,rel2_10,rel2_11,rel2_2,rel2_3,rel2_4,rel2_5,rel2_6,rel2_7,rel2_8,rel2_9,rel3_0,rel3_1,rel3_10,rel3_11,rel3_12,rel3_13,rel3_14,rel3_15,rel3_16,rel3_17,rel3_18,rel3_19,rel3_2,rel3_20,rel3_21,rel3_22,rel3_23,rel3_3,rel3_4,rel3_5,rel3_6,rel3_7,rel3_8,rel3_9,relt_0,relt_1,relt_2,rnaccess,rnusers,rtotcpu,s_0,s_1,s_2,s_3,s_4,size,tier,totcpu,wct
999669242,207737071.0,2186,20186,3,0,759090,14251.0,6,21675970,2158,72274,1,10,11.0,5862538,335429,30667701,373256,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0.6,0.4,3.9,0,3,8,0,0,8002,5,64280.0,216946588.0
332990665,114683734.0,2186,176521,3,1,759090,21311.0,88,334493030,32621,86197,1,4,8.0,6086362,968016,123342232,1037052,0,0,0,0,1,2,0,0,0,0,0,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,1,0,2,0.8,0.3,3.5,0,6,9,0,0,96689,3,58552.0,276683510.0
....
        </code>
    </pre>
2014 dataset is available at
<a href="https://git.cern.ch/web/CMS-DMWM-Analytics-data.git">
    https://git.cern.ch/web/CMS-DMWM-Analytics-data.git</a>
</small>

</section>

<section>
<h2>Dataframe description</h2>
<small>
<pre>
<span class="highlight">id:</span> unique id constructed as long('%s%s%s'%(tstamp,dbsinst,dataset_id)) % 2**30
<span class="highlight">cpu:</span> CPU time reported by Dashboard data-service for given dataset
<span class="highlight">creator:</span> anonymized DN of the user who created given dataset, reported by DBS
<span class="highlight">dataset:</span> DBS dataset id (comes from DBS APIs/database back-end)
<span class="highlight">dbs:</span> DBS instance id
<span class="highlight">dtype:</span> anonymized DBS data type (e.g. data, mc)
<span class="highlight">era:</span> anonymized DBS acquisition era name associated with given dataset
<span class="highlight">nblk:</span> number of blocks in given dataset, reported by DBS
<span class="highlight">nevt:</span> number of events in given dataset, reported by DBS
<span class="highlight">nfiles:</span> number of files in given dataset, reported by DBS
<span class="highlight">nlumis:</span> number of lumi sections in given dataset, reported by DBS
<span class="highlight">nrel:</span> number of releases associated with given dataset, reported by DBS
<span class="highlight">nsites:</span> number of sites associated with given dataset, reported by PhEDEx
<span class="highlight">parent:</span> parent id of given dataset, reported by DBS
<span class="highlight">primds:</span> anonymized primary dataset name, reported by DBS
<span class="highlight">proc_evts:</span> number of processed events, reported by Dashboard
<span class="highlight">procds:</span> anonymized processed dataset name, reported by DBS
<span class="highlight">rel1_N:</span> DBS release counter defined as N-number of series releases associated with given dataset
<span class="highlight">rel2_N:</span> DBS release counter defined as N-number of major releases associated with given dataset
<span class="highlight">rel3_N:</span> DBS release counter defined as N-number of minor releases associated with given dataset
<span class="highlight">s_X:</span> PhEDEx site counter, i.e. number of Tier sites holding this dataset replica
<span class="highlight">size:</span> size of the dataset, reported by DBS and normalized to GB metric
<span class="highlight">tier:</span> anonymized tier name, reported by DBS
<span class="highlight">wct:</span> wall clock counter for given dataset, reported by Dashboard

Target variables:
<span class="highlight">naccess:</span> number of accesses to a dataset, reported by PopularityDB
<span class="highlight">nusers:</span> number of users*days to a dataset, reported by PopularityDB
<span class="highlight">totcpu:</span> number of cpu hours to accessed dataset, reported by PopularityDB
<span class="highlight">rnaccess:</span> naccess(dataset)/SUM_i naccess(i), reported by PopularityDB
<span class="highlight">rnusers:</span> nusers(dataset)/SUM_i nusers(i), reported by PopularityDB
<span class="highlight">rtotcpu:</span> totcpu(dataset)/SUM_i totcpu(i), reported by PopularityDB
</pre>
<br />
<div align="left">
Some variables are useful for online learning while other can be used in offline context.
</div>
</small>
</section>

<section>
<h2>Live Data plots</h2>
<table><tr><td>
<img src="images/dbs1.gif" alt="DBS plots" />
    </td><td>
<img src="images/dbs3.gif" alt="DBS plots" />
</td></tr></table>
Data transition through 2014 on weekly basis
</section>

<section>
<h2>Correlations</h2>
<p><img src="images/corr.gif" alt="Correlations" /></p>
<small>
Subset of variables, showing all of them in single plot can hard to swallow.
</small>
</section>

<section>
<h2>Dataset popularity</h2>
<table><tr><td>
<img src="images/popularity.jpg" alt="Popularity" />
    </td><td>
<img src="images/naccess_cloud.gif" alt="Popular datasets" />
</td></tr></table>
<small>
Left plot shows few random datasets, while right one
summarizes 100 most accessed datasets through 2014.
<span class="highlight">Observation:</span>
dataset access is like stock market, i.e. time series, but
N(datasets) &gt;&gt; N(stocks &#64; NASDAQ)
</small>
</section>

<section>
<h2>Different dataset popularity metrics</h2>
<table><tr><td>
<img src="images/nusers_cloud.gif" alt="Popular datasets by nusers" />
    </td><td>
<img src="images/cpu_cloud.gif" alt="Popular datasets by cpu" />
</td></tr></table>
<small>
Left plot shows popular datasets by nusers, while right one
shows popular datasets by totcpu metric.
<br />
Therefore, target defition should be clearly defined. For the rest of slides I'll stick with naccess.
</small>
</section>

<section>
<h2>How to proceed</h2>
<div align="left">
    <ul>
        <li>Once we generated the data we want to learn from it and make prediction</li>
        <li>Transform and pivot the data to gain as mush as possible</li>
        <li>Use Machine Learning algorithms, e.g. regression, classification or online learning techniques to find best model</li>
        <li>Use different tools and libraries (R, python, VW, etc.)</li>
        <li>DCAFPilot project provides all necessary tools to do the job
            <ul>
                <li>generate data</li>
                <li>transform data</li>
                <li>build model and perform its validation</li>
                <li>acquire new data</li>
                <li>make predictions</li>
            </ul>
        </li>
    </ul>
</div>
</section>

<section>
<h2>How to train your model</h2>
<div align="left">
<small>
    <span class="highlight">DCAFPilot tools:</span>
    <code>model</code>
    <pre><code class="bash">
# use model script to train your model
prompt$ model --help

Usage: model.py [options]

Options:
  -h, --help            show this help message and exit
  --scaler=SCALER       model scalers: ['StandardScaler', 'MinMaxScaler'], default None
  --scorer=SCORER       model scorers: ['accuracy', 'adjusted_rand_score', 'average_precision',
                        'f1', 'log_loss', 'mean_absolute_error', 'mean_squared_error',
                        'precision', 'r2', 'recall', 'roc_auc'], default None
  --learner=LEARNER     model learners: ['AdaBoostClassifier', 'AdaBoostRegressor', 'BaggingClassifier',
                        'BaggingRegressor', 'BernoulliNB', 'DecisionTreeClassifier', 'DecisionTreeRegressor',
                        'ExtraTreesClassifier', 'ExtraTreesRegressor', 'GaussianNB', 'GradientBoostingClassifier',
                        'GradientBoostingRegressor', 'KNeighborsClassifier', 'LinearSVC', 'PCA',
                        'RandomForestClassifier', 'RandomForestRegressor', 'RidgeClassifier',
                        'SGDClassifier', 'SGDRegressor', 'SVC', 'SVR',
                        'lda_rfc', 'pca_knc', 'pca_rfc', 'pca_svc'], default RandomForestClassifier
  --learner-params=LPARAMS
                        model classifier parameters, supply via JSON
  --learner-help=LEARNER_HELP
                        Print learner description, default None
  --split=SPLIT         split level for train/validation, default 0.33
  --train-file=TRAIN    train file, default train.csv
  --newdata=NEWDATA     new data file, default None
  --idx=IDX             initial index counter, default 0
  --limit=LIMIT         number of rows to process, default -1 (everything)
  --verbose=VERBOSE     verbose output, default=0
  --predict=PREDICT     Prediction file name, default None
    </code></pre>
<span class="highlight">Note:</span>
model script offers 8 regressors and 18 classifiers from
<a href="http://scikit-learn.org/stable/">sklearn</a> ML library and it is
easy to add new ones.
</small>
</div>
</section>

<section>
<h2>Learner documentation</h2>
<div align="left">
<small>
    <pre><code class="bash">
prompt$ model --learner-help=RandomForestRegressor

RandomForestRegressor(bootstrap=True, compute_importances=None,
           criterion='mse', max_depth=None, max_features='auto',
           max_leaf_nodes=None, min_density=None, min_samples_leaf=1,
           min_samples_split=2, n_estimators=10, n_jobs=1, oob_score=False,
           random_state=None, verbose=0)
A random forest regressor.

    A random forest is a meta estimator that fits a number of classifying
    decision trees on various sub-samples of the dataset and use averaging
    to improve the predictive accuracy and control over-fitting.

    Parameters
    ----------
    n_estimators : integer, optional (default=10)
        The number of trees in the forest.

    criterion : string, optional (default="mse")
        The "function" to measure the quality of a split. The only supported
        criterion is "mse" for the mean squared error.
        Note: this parameter is tree-specific.

    max_features : int, float, string or None, optional (default="auto")
        The number of features to consider when looking for the best split:

        - If int, then consider `max_features` features at each split.
        - If float, then `max_features` is a percentage and
          `int(max_features * n_features)` features are considered at each
          split.
        - If "auto", then `max_features=n_features`.
        - If "sqrt", then `max_features=sqrt(n_features)`.
        - If "log2", then `max_features=log2(n_features)`.
        - If None, then `max_features=n_features`.
...
    </code></pre>
</small>
</div>
</section>

<section>
<h2>First results</h2>
<div align="left">
<small>
    <span class="highlight">DCAFPilot tools:</span>
    <code>merge_csv, model</code>
    <pre><code>
# get the data, we keep it sequre in separate CERN based repository
prompt$ git clone https://:@git.cern.ch/kerberos/CMS-DMWM-Analytics-data

# merge all 2014 files into single file
prompt$ merge_csv --fin=CMS-DMWM-Analytics-data/Popularity/DCAFPilot/data/0.0.2 --fout=2014.csv.gz --verbose

# Train the model
prompt$ model --learner=RandomForestRegressor --train-file=2014.csv.gz --scorer=r2

RandomForestRegressor(bootstrap=True, compute_importances=None,
           criterion='mse', max_depth=None, max_features='auto',
           max_leaf_nodes=None, min_density=None, min_samples_leaf=1,
           min_samples_split=2, n_estimators=10, n_jobs=1, oob_score=False,
           random_state=None, verbose=0)
Feature ranking:
1. importance 0.558986, feature nsites
2. importance 0.105061, feature s_2
3. importance 0.074761, feature rel3_0
4. importance 0.049544, feature proc_evts
5. importance 0.043663, feature s_1
6. importance 0.035363, feature rel3_2
7. importance 0.034260, feature s_3
8. importance 0.029350, feature rel3_4
9. importance 0.005171, feature relt_2
Score metric (r2): 0.897606379597

Definition: r2=1-Sres/Stot, Stot=SUM_i(Y_i-Y_hat)^2, Sres=SUM_i(F_i-Y_hat)^2
    </code></pre>
Does this good results? Are site variables good predictors? Should we use regressor or classification
model? ...
<br />
Too many things to test first, e.g. scaling, feature selection, model choice,
cross-validation, etc.
</small>
</div>
</section>

<section>
<h2>Other learners</h2>
<div align="left">
<small>
<span class="highlight">model</span> tool provides easy way to run
different classifiers. For full list see
<span class="highlight">model --help</span> option
    <pre>
        <code>
# run AdaBoost regressor model
prompt$ model --learner=AdaBoostRegressor --train-file=2014.csv.gz --scorer=r2 --scaler=StandardScaler
AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',
         n_estimators=50, random_state=None)
Score metric (r2): -0.943959694892

# run Bagging regressor model
prompt$ model --learner=BaggingRegressor --train-file=2014.csv.gz --scorer=r2 --scaler=StandardScaler
BaggingRegressor(base_estimator=None, bootstrap=True,
         bootstrap_features=False, max_features=1.0, max_samples=1.0,
         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,
         verbose=0)
Score metric (r2): -2.11591876017

        </code>
    </pre>

<br />
<span class="highlight">Observation:</span>
Should we restrict ourselves by regression model and specific package/language?
</small>
</div>
</section>

<!--

<section>
<h2>Online learning algorithm</h2>
<div align="left">
<small>
The <a href="http://hunch.net/~vw/">Vowpal wabbit</a> is online learning
algorithm started at Yahoo! Research and continuing at Microsoft Research to
design a fast, scalable, useful learning algorithm. VW is the essence of speed
in machine learning, able to learn from <u>terafeature</u> datasets with ease. Via
parallel learning, it can exceed the throughput of any single machine network
interface when doing linear learning, a first amongst learning algorithms.
<br />
<br />
<span class="highlight">DCAFPilot tools:</span>
<code>csv2vw</code>
    <pre>
prompt$ csv2vw --csv=2014.csv.gz --vw=2014.vw
prompt$ head -2 2014.vw

0.006 '999669242 |f size:8002 nevt:21675970 creator:2179 wct:216946588 dtype:0 dataset:20186 procds:373256 nfiles:2158 dbs:3 rel3_8:1 rel3_22:0 rel3_20:0 rel3_21:0 rel3_7:0 rel3_6:0 rel3_5:0 rel3_4:0 rel3_3:0 rel3_2:1 rel3_1:0 rel3_0:0 nsites:10 rel3_9:0 rel3_23:0 rel2_8:0 rel2_9:0 rel2_6:0 rel2_7:0 rel2_4:0 rel2_5:0 rel2_2:1 rel2_3:1 rel2_0:0 rel2_1:0 parent:5862538 rel1_1:0 rel1_0:0 rel1_3:0 rel1_2:0 rel1_5:1 rel1_4:1 rel1_7:0 nrel:1 rel3_17:0 rel3_16:0 rel3_15:0 nblk:6 rel3_13:0 rel3_12:0 rel3_11:0 rel3_10:0 tier:5 primds:335429 rel1_6:0 rel2_10:0 rel2_11:0 rel3_19:0 rel3_18:0 s_1:3 s_0:0 s_3:0 s_2:8 s_4:0 relt_0:1 relt_1:0 relt_2:1 rel3_14:0 era:759090 nlumis:72274 proc_evts:30667701 cpu:207737071

0.008 '332990665 |f size:96689 nevt:334493030 creator:2179 wct:276683510 dtype:1 dataset:176521 procds:1037052 nfiles:32621 dbs:3 rel3_8:1 rel3_22:0 rel3_20:0 rel3_21:0 rel3_7:1 rel3_6:0 rel3_5:0 rel3_4:0 rel3_3:0 rel3_2:1 rel3_1:0 rel3_0:0 nsites:4 rel3_9:0 rel3_23:0 rel2_8:0 rel2_9:0 rel2_6:0 rel2_7:0 rel2_4:0 rel2_5:0 rel2_2:1 rel2_3:2 rel2_0:0 rel2_1:0 parent:6086362 rel1_1:0 rel1_0:0 rel1_3:0 rel1_2:0 rel1_5:2 rel1_4:1 rel1_7:0 nrel:1 rel3_17:0 rel3_16:0 rel3_15:0 nblk:88 rel3_13:0 rel3_12:0 rel3_11:0 rel3_10:0 tier:3 primds:968016 rel1_6:0 rel2_10:0 rel2_11:0 rel3_19:0 rel3_18:0 s_1:6 s_0:0 s_3:0 s_2:9 s_4:0 relt_0:1 relt_1:0 relt_2:2 rel3_14:0 era:759090 nlumis:86197 proc_evts:123342232 cpu:114683734
    </pre>
    <span class="highlight">Please note:</span>
    VW should be installed externally, here we show its data format
and how to use it. The <b>csv2vw</b> script is provided by DCAFPilot package.
</small>
</div>
</section>

<section>
<h2>Online learning algorithm, cont'd</h2>
<div align="left">
<small>
    <span class="highlight">External tools:</span>
    <code>vw</code>
    <pre>
        <code>
# train our model
prompt$ vw -d 2014.vw -c -k -f model.vw
Num weight bits = 18, learning rate = 0.5, initial_t = 0, power_t = 0.5
average    since         example     example  current  current  current
loss       last          counter      weight    label  predict features
0.000036   0.000036            1         1.0   0.0060   0.0000       29
...
0.000014   0.000010        65536     65536.0   0.0010   0.0000       46

finished run, number of examples per pass = 113908, passes used = 1
average loss = 1.29019e-05
best constants loss = 0.000366653
total feature number = 6071657

# make prediction
prompt$ vw -d new.vw -t -i model.vw -p preds.txt

# convert VW prediction into CSV format
prompt$ vw_pred2csv --fin=preds.txt --fout=preds.csv

# check VW prediction against data sample
prompt$ check_prediction --fin=dataframe-20141119-20141126.csv.gz --fpred=preds.csv
        </code>
                         RandomForestRegressor   VW regression
Explained variance score: 0.898592908177         0.341559976762
Mean absolute error     : 0.000386050647644      0.000349310257493
Mean squared error      : 2.42792029612e-06      1.54817975922e-05
R2 score                : 0.896133210959         0.337686411265
    </pre>
</small>
</div>
</section>

-->

<section>
<h2>Use R and do classification</h2>
<div align="left">
<small>
<span class="highlight">External tools:</span>
<code>R-language, e1071 library (Support Vector Machine classification)</code>
    <pre>
        <code class="python">
library(e1071)

csv_files <- list.files(path="./", full.names=TRUE, pattern="dataframe-.*.csv.gz")
df <- read.csv(file=csv_files[1])
for ( i in 2:length(csv_files) ) {
  df <- rbind(df,read.csv(file=csv_files[1]))
}

# remove variables which will not be useful for predicting _new_ data popularity
df <- df[,cbind("cpu","creator","dtype","era","nblk","nevt","nfiles","nlumis",
                "parent","primds","proc_evts","procds","size","tier","wct","target")]

# define classifcation
df$target <- df$target > 0.001
dLen <- nrow(df)
nTrain <- floor(dLen * 0.8)
dTrain <- df[1:nTrain,]
dTest  <- df[nTrain+1:(dLen-nTrain-1),]

# The default cost=1 gives only 90% accuracty, 2611 support vectors.
# cost=10 gives 97% accuracy with 1012 support vectors.
# cost=100 gives 100% accuracy with only 431 support vectors.
# putting the cost to infinity (10^6) gives 285 support vectors.
s <- svm(target ~ ., data=dTrain, type="C-classification", cost=50)
print(s) # 542 support  for cost=50

yTest <- predict(s,dTest)
print("Confusion matrix for test sample")
print(table(yTest,dTest$target))
        </code>
    </pre>
</small>
</div>
</section>

<section>
<h2>From data to prediction</h2>
<div align="left">
<ol>
    <li>Generate dataframe or get it from existing repository</li>
    <li>Transform data into suitable format for ML</li>
    <li>Build ML model
        <ul>
            <li>use classification or regression techniques</li>
            <li>train and validate your model
                <ul>
                    <li>split data into train and validation sets
                    <br/>
                    we have ~600K rows in 2014 dataset
                    <br/>
                    train set (Jan-Nov), test set (Dec)
                    <li>estimate your predictive power on validation set</li>
                </ul>
            </li>
        </ul>
    </li>
    <li>Generate new data and transform it similar to step #2.</li>
    <li>Apply your best model to new data to make prediction</li>
    <li>Verify prediction with popularity DB once data metrics become available</li>
</ol>
</div>
</section>

<section data-background="#999999" data-background-transition="zoom">
<h2>From data to prediction, step 1-3</h2>
<div align="left">
<small>
<span class="highlight">DCAFPilot tools:</span>
<code>merge_csv, model, check_prediction, pred2dataset</code>
</small>
    <pre>
        <code class="bash">
# get the data, we keep it secure in separate CERN based repository
prompt_1$ git clone https://:@git.cern.ch/kerberos/CMS-DMWM-Analytics-data

# merge dataframes, then split 2014.csv.gz into train/valid datasets
prompt_2$ merge_csv --fin=CMS-DMWM-Analytics-data/Popularity/DCAFPilot/data/0.0.3 --fout=2014.csv.gz --verbose

# transform data into classification problem and remove useless variables
prompt_3$ transform_csv --fin=2014.csv.gz --fout=train_clf.csv.gz --target=naccess --target-thr=100 \
          --drops=nusers,totcpu,rnaccess,rnusers,rtotcpu,nsites,s_0,s_1,s_2,s_3,s_4,wct

# train the model
prompt_4$ model --learner=RandomForestClassifier --idcol=id --target=target --train-file=train_clf.csv.gz \
          --scaler=StandardScaler --newdata=valid_clf.csv.gz --predict=pred.txt

# check prediction
prompt_5$ check_prediction --fin=valid_clf.csv.gz --fpred=pred.txt --scorer=accuracy,precision,recall,f1
Score metric (precision_score): 0.79773214833
Score metric (accuracy_score): 0.982348203499
Score metric (recall_score): 0.952781844802
Score metric (f1_score): 0.868390325271
Precision=TP/(TP+FP), Accuracy=(TP+TN)/(TP+TN+FP+FN), Recall=TP/(TP+FN), F1=2*Precision*Recall/(Precision+Recall)

# convert prediction into human/CMS data format
prompt_6$ pred2dataset --fin=pred.txt --fout=pred.txt.out

# inspect prediction
prompt_7$ head -2 pred.txt.out
1.000,/GenericTTbar/HC-CMSSW_7_0_4_START70_V7-v1/GEN-SIM-RECO
1.000,/SingleMu/Run2012D-22Jan2013-v1/AOD
        </code>
    </pre>
</div>

</section>

<section data-background="#B3B3B3" data-background-transition="zoom">
<h2>Making predictions, steps 4-6</h2>
<div align="left">
<small>
<span class="highlight">DCAFPilot tools:</span>
<code>dataframe, transform_csv, model, pred2dataset, popular_datasets, verify_predictions</code>
</small>
<div>
    <pre>
        <code class="bash">
# seed dataset cache (w/ MongoDB back-end)
prompt_1$ dataframe --seed-cache --verbose=1

# get new data from DBS (you may need to run it in background)
prompt_2$ dataframe --start=20150101 --stop=20150108 --newdata --verbose=1 --fout=new-20150101-20150108.csv

# transform new data into classification problem similar to our train data
prompt_3$ transform_csv --fin=new-20150101-20150108.csv.gz --fout=newdata-20150101-20150108.csv.gz --target=naccess \
          --target-thr=100 --drops=nusers,totcpu,rnaccess,rnusers,rtotcpu,nsites,s_0,s_1,s_2,s_3,s_4,wct

# run the model with new data
prompt_4$ model --learner=RandomForestClassifier --idcol=id --target=target --train-file=train_clf.csv.gz \
        --scaler=StandardScaler --newdata=newdata-20150101-20150108.csv.gz --predict=pred.txt

# produce human readable format and inspect its output
prompt_5$ pred2dataset --fin=pred.txt --fout=pred.txt.out
prompt_6$ head -2 pred.txt.out
0.000,/RelValQCDForPF_14TeV/CMSSW_6_2_0_SLHC22_patch1-PH2_1K_FB_V6_UPG23SHNoTaper-v1/GEN-SIM-DIGI-RAW
0.000,/RelValQCDForPF_14TeV/CMSSW_6_2_0_SLHC22_patch1-PH2_1K_FB_V6_UPG23SHNoTaper-v1/DQMIO

# get popular datasets from popularity DB
prompt_7$ popular_datasets --start=20150101 --stop=20150108 > popdb-20150101-20150108.txt

# verify our prediction against similar period from popularity DB
prompt_8$ verify_predictions --pred=pred.txt.out --popdb=popdb-20150101-20150108.txt
Popular datasets   : 841
Predicted datasets : 187
Wrongly predicted  : 0
        </code>
    </pre>
</div>
</div>

</section>
<section>
<h2>Conclusions & Future directions</h2>
<div align="left">
<small>
<ul>
    <li>We show the proof of concept how to predict dataset popularity based on existing CMS tools
    <ul>
        <li>DCAFPilot package has main components to do the work:
        <span class="highlight">dataframe, transform_csv, merge_csv, model,
            pred2dataset, popular_datasets, check_prediction, verify_predictions</span> 
        </li>
    </ul>
    <br />
    </li>
    <li>We succeed making sensible prediction with simple RF model
    <ul>
        <li>Even though initial dataframe/model shows <span style="color:#FF6666">some potential</span>
        it should be <span style="color:#FF6666">thoughtfully studied</span> to avoid
        main ML obstacles, e.g. data memorization, over-fitting, etc., and
        <span style="color:#FF6666">checked with new data</span>
        </li>
        <li>More data in terms of volume and attributes may be required for
        further analysis, e.g. find physicists clustering on certain topics</li>
        <li>Even though all work was done on a single node with existing APIs we may need
        to pursue other approaches, e.g. ORACLE-Hadoop mapping, etc.</li>
    </ul>
    <br />
    </li>

    <li>
    Explore various ML algorithms: python, R, online-learning
    <br />
    <br />
    </li>

    <li>
    Try out different popularity metrics, e.g. naccess, ncpu, nusers or any
    combination of them
    <br />
    <br />
    </li>

    <li>
    Explore different approaches: track individual datasets, dataset groups, etc.
    <br />
    <br />
    </li>

    <li>
    Use other resources: user activity on HN, conference deadlines influence, etc.
    <br />
    <br />
    </li>

    <li>
    Test predictions with real data, i.e. acquire new datasets and make prediction for them,
    then wait for data from popularity DB and compare prediction with actual data
    <br />
    <br />
    </li>

    <li>Automate tools, e.g. weekly crontabs, generate model updates, verify model predictions</li>
</ul>
</small>
</div>
</section>

<!--
<section>
<h1>Happy Data Mining</h1>
</section>
-->

</div> <!-- end of reveal -->

</div> <!-- end of slides -->

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.min.js"></script>

<script>

// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: true,
    slideNumber: true,

    theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Optional libraries used to extend on reveal.js
    dependencies: [
        { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
        { src: 'plugin/markdown/showdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
        { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
        // { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
        // { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
    ]
});

</script>

</body>
</html>

