<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>DCAFPilot - pilot project for CMS computing data-mining</title>

    <meta name="description" content="DCAFPilot for CMS">
    <meta name="author" content="Valentin Kuznetsov">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<!--
    <link rel="stylesheet" href="css/reveal.min.css">
-->
    <link rel="stylesheet" href="css/reveal.css">
    <!-- beige.css,moon.css,serif.css,sky.css,default.css,night.css,simple.css,solarized.css -->
    <link rel="stylesheet" href="css/theme/default.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- local styles-->
    <style type="text/css" media="all">
    .highlight {color:#FF8000; padding: 30px;}
    </style>

    <!-- If the query includes 'print-pdf', use the PDF print sheet -->
    <script>
        document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">

<!-- Any section element inside of this container is displayed as a slide -->
<div class="slides">

<section>
    <h2>DCAFPilot for CMS</h2>
    <h3>pilot project for CMS computing data-mining</h3>
    <p>
    <small>by
        <a href="http://github.com/vkuznet">Valentin Kuznetsov</a> /
        <a href="http://www.cornell.edu">Cornell University</a>
    </small>
    </p>
</section>

<section>
    <h2>Talk outlines</h2>
    <ul>
        <li>Project scope</li>
        <li>Project outlines</li>
        <li>From data to prediction
            <ul>
                <li>gather information from data-services</li>
                <li>prepare information suitable for data analysis</li>
                <li>train learner algorithm</li>
                <li>make prediction</li>
            </ul>
        </li>
        <li>Future direction</li>
    </ul>
</section>

<section>
<h2>Project scope</h2>
<div align="left">
<ul>
<li>
DMWM-Analytics
(<a href="mailto:cms-dmwm-analytics@cern.ch">cms-dmwm-analytics@cern.ch</a>)
group would like to improve our understanding of CMS computing data,
full list of projects:
<small>
<a href="https://twiki.cern.ch/twiki/bin/viewauth/CMS/CMSComputingAnalytics">
https://twiki.cern.ch/twiki/bin/viewauth/CMS/CMSComputingAnalytics
</a>
</small>
</li>

<li>
Ultimately we'd like to learn from CMS data and make prediction to improve our resource utilization.
<br />
<br />
</li>

<li>
Initial goal is to predict popularity of new datasets.
<br />
<br />
</li>

<li>
Start with understanding metrics, analysis workflow, tools:
<ul>
    <li>
    <a href="https://github.com/dmwm/DMWMAnalytics/tree/master/Popularity/DCAFPilot">DCAFPilot</a>
    (Data and Computing Analysis Framework) is a pilot project to understand
    machinery involved with this problem.
    </li>
</ul>
</li>

</ul>

</div>
</section>

<section>
<h2>Why we need this</h2>
<div align="left">
    <ul>
        <li>CMS has Dynamic Data Placement group which uses historical
        information to place popular dataset to sites</li>
        <li>We would like to predict which datasets will become popular
        once they appear on a market</li>
        <li>We can extend the scope of previous task to predict
        decline in popularity of certain datasets, reduce redundant activity,
        improve resource allocation, etc.</li>
    </ul>
</div>
</section>

<section>
<h2>Project outlines</h2>
<div align="left">
    <p>
The DCAFPilot consists of several components:
<small>
    <ul>
    <li>Dataframe generator toolkit: collect/transform data from
    CMS data-services (DBS/PhEDEx/SiteDB/PopularityDB/Dashboard)
    and extract necessary bits for datasets in questions
    <br />
    <br />
    </li>
    <li>Machine Learning (ML) algorithms (python/R code) for data analysis
    <br />
    <br />
    </li>
    <li>Data manipulation scripts: merge, transform, check predictions, etc.</li>
    </ul>
</small>
</p>

<p>
Get the code:
<pre>
git clone git@github.com:dmwm/DMWMAnalytics.git
</pre>
</p>

<p>
Dependencies:
<small>
    <ul>
        <li><a href="https://github.com/dmwm/DMWMAnalytics/tree/master/Popularity/DCAFPilot">DCAFPilot</a> project, available at github</li>
        <li><a href="http://www.mongodb.org">MongoDB</a> for internal cache</li>
        <li>Python tools: <a href="http://pandas.pydata.org/">pandas</a>,
            <a href="http://www.numpy.org/">NumPy</a>, <a href="http://www.scipy.org/">SciPy</a>,
            <a href="http://scikit-learn.org/stable">sklearn</a> ML toolkit</li>
        <li>
        <span class="highlight">Optional:</span>
        <a href="http://www.r-project.org">R language</a> for data exploration and ML algorithms,
        <a href="https://github.com/EducationalTestingService/skll">SKLL</a>
        toolkit to run/experiment with various ML algorithms, any other external
        ML toolkits, e.g.
        <a href="https://github.com/JohnLangford/vowpal_wabbit">Vowpal Wabbit</a> online-learning algorithm</li>
    </ul>
</small>
</p>

</div>
</section>

<section>
<h2>Data collection flow</h2>
<div align="left">
<ul>
<li>
Collect data via the following set of rules
<small>
    <ul>
    <li>Collect all datasets (4 DBS instances) into internal cache</li>
    <li>Collection popular datasets from PopularityDB on weekly basis</li>
    <li>Get summary for datasets from DBS/PhEDEx/SiteDB/Dashboard data-services</li>
    <li>Complement dataframe with random set of DBS datasets which were not
    visible in popularity for given time interval</li>
    </ul>
</small>
<br />
<br />
</li>

<li>
CMS data-service APIs used by DCAFPilot package
<br />
<small>
    <ul>
        <li><a href="https://cmsweb.cern.ch/dbs/">DBS:</a> datasets, releases, filesummaries, releaseversions, datasetparents APIs</li>
        <li><a href="https://cmsweb.cern.ch/phedex">PhEDEx:</a> blockReplicas API</li>
        <li><a href="https://cmsweb.cern.ch/sitedb">SiteDB:</a> site-names, people APIs</li>
        <li><a href="https://cms-popularity.cern.ch/popdb/popularity/apidoc">PopularityDB:</a> DSStatInTimeWindow API</li>
        <li><a href="https://twiki.cern.ch/twiki/bin/view/ITAnalyticsWorkingGroup/ExperimentDashboard">Dashboard:</a> jobefficiencyapi API</li>
    </ul>
</small>

</li>
</ul>

</div>
</section>

<section>
<h2>Data collection flow diagram</h2>
<p><img src="images/DCAFPilot_flow.png" alt="Data Collection Flow" width="70%" style="background-color:#E6E6E6" /></p>
</section>

<section>
<h2>Dataframe preparation, cont'd</h2>
<div align="left">
    <pre>
Queried 5 CMS data-services: DBS, PhEDEx, SiteDB, PopularityDB, Dashboard
   - used 10 APIs to get data content
   - feed internal cache with ~220K datasets from 4 DBS instances,
     ~900 release names, 500+ site names, ~5k people DNs.
   - placed ~800K queries

The final dataframe is constructed out of 78 variables and has 52 files and ~600K rows
   - each file is worth of 1 week of CMS data, ~600KB zipped/file
   - each file has about ~1K of popular datasets plus 10K random "un-popular" datasets

Elapsed time: ~4h to 1h per job, times fade out due to cache usage (MongoDB)
All jobs run on two CERN VM w/ N jobs/core splitting
    </pre>
<small>
We anonymized all data and performed factorization via internal cache
    <pre>
        <code>
id,cpu,creator,dataset,dbs,dtype,era,naccess,nblk,nevt,nfiles,nlumis,nrel,nsites,nusers,parent,primds,proc_evts,procds,rel1_0,rel1_1,rel1_2,rel1_3,rel1_4,rel1_5,rel1_6,rel1_7,rel2_0,rel2_1,rel2_10,rel2_11,rel2_2,rel2_3,rel2_4,rel2_5,rel2_6,rel2_7,rel2_8,rel2_9,rel3_0,rel3_1,rel3_10,rel3_11,rel3_12,rel3_13,rel3_14,rel3_15,rel3_16,rel3_17,rel3_18,rel3_19,rel3_2,rel3_20,rel3_21,rel3_22,rel3_23,rel3_3,rel3_4,rel3_5,rel3_6,rel3_7,rel3_8,rel3_9,relt_0,relt_1,relt_2,rnaccess,rnusers,rtotcpu,s_0,s_1,s_2,s_3,s_4,size,tier,totcpu,wct
999669242,207737071.0,2186,20186,3,0,759090,14251.0,6,21675970,2158,72274,1,10,11.0,5862538,335429,30667701,373256,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0.6,0.4,3.9,0,3,8,0,0,8002,5,64280.0,216946588.0
332990665,114683734.0,2186,176521,3,1,759090,21311.0,88,334493030,32621,86197,1,4,8.0,6086362,968016,123342232,1037052,0,0,0,0,1,2,0,0,0,0,0,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,1,0,2,0.8,0.3,3.5,0,6,9,0,0,96689,3,58552.0,276683510.0
....
        </code>
    </pre>
2014 dataset is available at
<a href="https://git.cern.ch/web/CMS-DMWM-Analytics-data.git">
    https://git.cern.ch/web/CMS-DMWM-Analytics-data.git</a>
</small>

</section>

<section>
<h2>Dataframe description</h2>
<small>
<pre>
<span class="highlight">id:</span> unique id constructed as long('%s%s%s'%(tstamp,dbsinst,dataset_id)) % 2**30
<span class="highlight">cpu:</span> CPU time reported by Dashboard data-service for given dataset
<span class="highlight">creator:</span> anonymized DN of the user who created given dataset, reported by DBS
<span class="highlight">dataset:</span> DBS dataset id (comes from DBS APIs/database back-end)
<span class="highlight">dbs:</span> DBS instance id
<span class="highlight">dtype:</span> anonymized DBS data type (e.g. data, mc)
<span class="highlight">era:</span> anonymized DBS acquisition era name associated with given dataset
<span class="highlight">nblk:</span> number of blocks in given dataset, reported by DBS
<span class="highlight">nevt:</span> number of events in given dataset, reported by DBS
<span class="highlight">nfiles:</span> number of files in given dataset, reported by DBS
<span class="highlight">nlumis:</span> number of lumi sections in given dataset, reported by DBS
<span class="highlight">nrel:</span> number of releases associated with given dataset, reported by DBS
<span class="highlight">nsites:</span> number of sites associated with given dataset, reported by PhEDEx
<span class="highlight">parent:</span> parent id of given dataset, reported by DBS
<span class="highlight">primds:</span> anonymized primary dataset name, reported by DBS
<span class="highlight">proc_evts:</span> number of processed events, reported by Dashboard
<span class="highlight">procds:</span> anonymized processed dataset name, reported by DBS
<span class="highlight">rel1_N:</span> DBS release counter defined as N-number of series releases associated with given dataset
<span class="highlight">rel2_N:</span> DBS release counter defined as N-number of major releases associated with given dataset
<span class="highlight">rel3_N:</span> DBS release counter defined as N-number of minor releases associated with given dataset
<span class="highlight">s_X:</span> PhEDEx site counter, i.e. number of Tier sites holding this dataset replica
<span class="highlight">size:</span> size of the dataset, reported by DBS and normalized to GB metric
<span class="highlight">tier:</span> anonymized tier name, reported by DBS
<span class="highlight">wct:</span> wall clock counter for given dataset, reported by Dashboard

Target variables:
<span class="highlight">naccess:</span> number of accesses to a dataset, reported by PopularityDB
<span class="highlight">nusers:</span> number of users*days to a dataset, reported by PopularityDB
<span class="highlight">totcpu:</span> number of cpu hours to accessed dataset, reported by PopularityDB
<span class="highlight">rnaccess:</span> naccess(dataset)/SUM_i naccess(i), reported by PopularityDB
<span class="highlight">rnusers:</span> nusers(dataset)/SUM_i nusers(i), reported by PopularityDB
<span class="highlight">rtotcpu:</span> totcpu(dataset)/SUM_i totcpu(i), reported by PopularityDB
</pre>
<br />
<div align="left">
Some variables are useful for online learning while other can be used in offline context.
</div>
</small>
</section>

<section>
<h2>Live Data plots</h2>
<table><tr><td>
<img src="images/dbs1.gif" alt="DBS plots" />
    </td><td>
<img src="images/dbs3.gif" alt="DBS plots" />
</td></tr></table>
Data transition through 2014 on weekly basis
</section>

<section>
<h2>Correlations</h2>
<p><img src="images/corr.gif" alt="Correlations" /></p>
<small>
Subset of variables, showing all of them in single plot can hard to swallow.
</small>
</section>

<section>
<h2>Dataset popularity</h2>
<table><tr><td>
<img src="images/popularity.jpg" alt="Popularity" />
    </td><td>
<img src="images/naccess_cloud.gif" alt="Popular datasets" />
</td></tr></table>
<small>
Left plot shows few random datasets, while right one
summarizes 100 most accessed datasets through 2014.
<span class="highlight">Observation:</span>
dataset access is like stock market, but
N(datasets) &gt;&gt; N(stocks &#64; NASDAQ)
</small>
</section>

<section>
<h2>Different dataset popularity metrics</h2>
<table><tr><td>
<img src="images/nusers_cloud.gif" alt="Popular datasets by nusers" />
    </td><td>
<img src="images/cpu_cloud.gif" alt="Popular datasets by cpu" />
</td></tr></table>
<small>
<span class="highlight">Left:</span> popular datasets by nusers,
<span class="highlight">Right:</span> popular datasets by totcpu metric.
<br />
Therefore, target defition should be clearly defined.
For the rest of slides I'll stick with <span style="color:#BAFFE9"><u>naccess</u></span> metric.
</small>
</section>

<section>
<h2>From data to prediction</h2>
<div align="left">
<ol>
    <li>Generate dataframe or get it from existing repository</li>
    <li>Transform data into suitable format for ML</li>
    <li>Build ML model
        <ul>
            <li>use classification or regression techniques</li>
            <li>train and validate your model
                <ul>
                    <li>split data into train and validation sets
                    <br/>
                    we have ~600K rows in 2014 dataset
                    <br/>
                    train set (Jan-Nov), test set (Dec)
                    <li>estimate your predictive power on validation set</li>
                </ul>
            </li>
        </ul>
    </li>
    <li>Generate new data and transform it similar to step #2.</li>
    <li>Apply your best model to new data to make prediction</li>
    <li>Verify prediction with popularity DB once data metrics become available</li>
</ol>
</div>
</section>

<section data-background="#999999" data-background-transition="zoom">
<h2>From data to prediction, step 1-3</h2>
<div align="left">
<small>
<span class="highlight">DCAFPilot tools:</span>
<code>merge_csv, model, check_prediction, pred2dataset</code>
</small>
    <pre>
        <code class="bash">
# get the data, we keep it secure in separate CERN based repository
prompt_1$ git clone https://:@git.cern.ch/kerberos/CMS-DMWM-Analytics-data

# merge dataframes, then split 2014.csv.gz into train/valid datasets
prompt_2$ merge_csv --fin=CMS-DMWM-Analytics-data/Popularity/DCAFPilot/data/0.0.3 --fout=2014.csv.gz --verbose

# transform data into classification problem and remove useless variables
prompt_3$ transform_csv --fin=2014.csv.gz --fout=train_clf.csv.gz --target=naccess --target-thr=100 \
          --drops=nusers,totcpu,rnaccess,rnusers,rtotcpu,nsites,s_0,s_1,s_2,s_3,s_4,wct

# train the model
prompt_4$ model --learner=RandomForestClassifier --idcol=id --target=target --train-file=train_clf.csv.gz \
          --scaler=StandardScaler --newdata=valid_clf.csv.gz --predict=pred.txt

# check prediction
prompt_5$ check_prediction --fin=valid_clf.csv.gz --fpred=pred.txt --scorer=accuracy,precision,recall,f1
Score metric (precision_score): 0.79773214833
Score metric (accuracy_score): 0.982348203499
Score metric (recall_score): 0.952781844802
Score metric (f1_score): 0.868390325271
Precision=TP/(TP+FP), Accuracy=(TP+TN)/(TP+TN+FP+FN), Recall=TP/(TP+FN), F1=2*Precision*Recall/(Precision+Recall)

# convert prediction into human/CMS data format
prompt_6$ pred2dataset --fin=pred.txt --fout=pred.txt.out

# inspect prediction
prompt_7$ head -2 pred.txt.out
1.000,/GenericTTbar/HC-CMSSW_7_0_4_START70_V7-v1/GEN-SIM-RECO
1.000,/SingleMu/Run2012D-22Jan2013-v1/AOD
        </code>
    </pre>
</div>

</section>

<section data-background="#B3B3B3" data-background-transition="zoom">
<h2>Making predictions, steps 4-6</h2>
<div align="left">
<small>
<span class="highlight">DCAFPilot tools:</span>
<code>dataframe, transform_csv, model, pred2dataset, popular_datasets, verify_predictions</code>
</small>
<div>
    <pre>
        <code class="bash">
# seed dataset cache (w/ MongoDB back-end)
prompt_1$ dataframe --seed-cache --verbose=1

# get new data from DBS (you may need to run it in background)
prompt_2$ dataframe --start=20150101 --stop=20150108 --newdata --verbose=1 --fout=new-20150101-20150108.csv

# transform new data into classification problem similar to our train data
prompt_3$ transform_csv --fin=new-20150101-20150108.csv.gz --fout=newdata-20150101-20150108.csv.gz --target=naccess \
          --target-thr=100 --drops=nusers,totcpu,rnaccess,rnusers,rtotcpu,nsites,s_0,s_1,s_2,s_3,s_4,wct

# run the model with new data
prompt_4$ model --learner=RandomForestClassifier --idcol=id --target=target --train-file=train_clf.csv.gz \
        --scaler=StandardScaler --newdata=newdata-20150101-20150108.csv.gz --predict=pred.txt

# produce human readable format and inspect its output
prompt_5$ pred2dataset --fin=pred.txt --fout=pred.txt.out
prompt_6$ head -2 pred.txt.out
0.000,/RelValQCDForPF_14TeV/CMSSW_6_2_0_SLHC22_patch1-PH2_1K_FB_V6_UPG23SHNoTaper-v1/GEN-SIM-DIGI-RAW
0.000,/RelValQCDForPF_14TeV/CMSSW_6_2_0_SLHC22_patch1-PH2_1K_FB_V6_UPG23SHNoTaper-v1/DQMIO

# get popular datasets from popularity DB
prompt_7$ popular_datasets --start=20150101 --stop=20150108 > popdb-20150101-20150108.txt

# verify our prediction against similar period from popularity DB
prompt_8$ verify_predictions --pred=pred.txt.out --popdb=popdb-20150101-20150108.txt
Popular datasets   : 841
Predicted datasets : 187
Wrongly predicted  : 0
        </code>
    </pre>
</div>
</div>
</section>

<section>
<h2>Discussion</h2>
<div align="left">
    <ul>
        <li>Shown steps demonstrate ability of DCAFPilot project</li>
        <li>The results should be take with caution
        <ul>
            <li>New data corresponded to first week of the year when there were no "real" activity among physicists</li>
            <li>Chosen naccess metric may have bias towards test datasets which should be discarded</li>
            <li>We may need to scan metric space for suitable definition of dataset "popularity"</li>
        </ul>
        </li>
        <li>Use rolling approach: get new data &#8594; adjust model &#8594; make prediction and repeat the cycle</li>
        <li>We may need to extend existing dataframe to new dimensions: cluster users activity via HN analysis, conference dates; analysis of release quality, etc.</li>
    </ul>
</div>
</section>

<section>
<h2>Conclusions & Future directions</h2>
<div>
<small>
    <table style="font-size:small; font-family:monospace;">
        <tr>
            <td>Classifier</td>
            <td>naccess&gt;100</td>
            <td>naccess&gt;0</td>
            <td>naccess&gt;10 and naccess&lt;10000 and nsites&lt;50</td>
        </tr>

        <tr style="background-color: #7f7f7f;">
            <td>RF</td>
            <td>
pr: 0.79 <br />
ac: 0.98 <br />
re: 0.95 <br />
f1: 0.86 <br />
            </td>

            <td>
pr: 0.87 <br />
ac: 0    <br />
re: 0    <br />
f1: 0    <br />
            </td>

            <td>
pr: 0.89 <br />
ac: 0    <br />
re: 0    <br />
f1: 0    <br />
            </td>
        </tr>

        <tr style="background-color:#999999;">
            <td>SGD</td>
            <td>
pr: 0.64 <br />
ac: 0.15 <br />
re: 0.76 <br />
f1: 0.26 <br />
            </td>

            <td>
pr: 0.20 <br />
ac: 0.13 <br />
re: 0.97 <br />
f1: 0.23 <br />
            </td>

            <td>
pr: 0.18 <br />
ac: 0.06 <br />
re: 0.49 <br />
f1: 0.11 <br />
            </td>
        </tr>

        <tr style="background-color: #7f7f7f;">
            <td>Gaussian Naive Bayes</td>
            <td>
pr: 0.88 <br />
ac: 0.39 <br />
re: 0.81 <br />
f1: 0.53 <br />
            </td>

            <td>
pr: 0.81 <br />
ac: 0.38 <br />
re: 0.71 <br />
f1: 0.50 <br />
            </td>

            <td>
pr: 0.89 <br />
ac: 0.53 <br />
re: 0.43 <br />
f1: 0.47 <br />
            </td>
        </tr>

        <tr style="background-color: #999999;">
            <td>Linear SVC</td>
            <td>
pr: 0.87 <br />
ac: 0.35 <br />
re: 0.61 <br />
f1: 0.44 <br />
            </td>

            <td>
pr: 0.24 <br />
ac: 0.14 <br />
re: 0.96 <br />
f1: 0.24 <br />
            </td>

            <td>
pr: 0.18 <br />
ac: 0.06 <br />
re: 0.48 <br />
f1: 0.11 <br />
            </td>
        </tr>
    </table>
</small>
</div>
</section>

<section>
<h2>Conclusions & Future directions</h2>
<div align="left">
<small>
<ul>
    <li>We show the proof of concept how to predict dataset popularity based on existing CMS tools
    <ul>
        <li>DCAFPilot package has main components to do the work:
        <span class="highlight">dataframe, transform_csv, merge_csv, model,
            pred2dataset, popular_datasets, check_prediction, verify_predictions</span>
        </li>
    </ul>
    <br />
    </li>
    <li>We succeed making sensible prediction with simple RF model
    <ul>
        <li>Even though initial dataframe/model shows <span style="color:#FF6666">some potential</span>
        it should be <span style="color:#FF6666">thoughtfully studied</span> to avoid
        main ML obstacles, e.g. data memorization, over-fitting, etc., and
        <span style="color:#FF6666">checked with new data</span>
        </li>
        <li>More data in terms of volume and attributes may be required for
        further analysis, e.g. find physicists clustering on certain topics</li>
        <li>Even though all work was done on a single node with existing APIs we may need
        to pursue other approaches, e.g. ORACLE-Hadoop mapping, etc.</li>
    </ul>
    <br />
    </li>

    <li>
    Explore various ML algorithms: python, R, online-learning
    <br />
    <br />
    </li>

    <li>
    Try out different popularity metrics, e.g. (r)naccess, (r)totcpu, (r)nusers or any
    combination of them
    <br />
    <br />
    </li>

    <li>
    Explore different approaches: track individual datasets, dataset groups, etc.
    <br />
    <br />
    </li>

    <li>
    Use other resources: user activity on HN, conference deadlines influence, etc.
    <br />
    <br />
    </li>

    <li>
    Test predictions with real data, i.e. acquire new datasets and make prediction for them,
    then wait for data from popularity DB and compare prediction with actual data
    <br />
    <br />
    </li>

    <li>Automate tools, e.g. weekly crontabs, generate model updates, verify model predictions</li>
</ul>
</small>
</div>
</section>

<!--
<section>
<h1>Happy Data Mining</h1>
</section>
-->

</div> <!-- end of reveal -->

</div> <!-- end of slides -->

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.min.js"></script>

<script>

// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: true,
    slideNumber: true,

    theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Optional libraries used to extend on reveal.js
    dependencies: [
        { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
        { src: 'plugin/markdown/showdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
        { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
        // { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
        // { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
    ]
});

</script>

</body>
</html>

